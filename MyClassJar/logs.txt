
==> Audit <==
|-----------|--------------------------------|----------|----------------|---------|---------------------|---------------------|
|  Command  |              Args              | Profile  |      User      | Version |     Start Time      |      End Time       |
|-----------|--------------------------------|----------|----------------|---------|---------------------|---------------------|
| start     |                                | minikube | PARADISE\KUHAN | v1.35.0 | 07 May 25 14:52 IST |                     |
| start     |                                | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 09:15 IST | 09 May 25 09:18 IST |
| start     |                                | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 14:28 IST | 09 May 25 14:29 IST |
| delete    |                                | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:08 IST | 09 May 25 15:08 IST |
| start     |                                | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:09 IST | 09 May 25 15:10 IST |
| image     | load                           | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:10 IST | 09 May 25 15:10 IST |
|           | kuhancdac/kmyclassjar:latest   |          |                |         |                     |                     |
| addons    | enable dashboard               | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:10 IST | 09 May 25 15:10 IST |
| stop      |                                | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:10 IST | 09 May 25 15:10 IST |
| start     |                                | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:10 IST | 09 May 25 15:11 IST |
| delete    |                                | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:19 IST | 09 May 25 15:19 IST |
| start     |                                | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:19 IST | 09 May 25 15:20 IST |
| image     | load                           | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:20 IST | 09 May 25 15:21 IST |
|           | kuhancdac/kmyclassjar:latest   |          |                |         |                     |                     |
| addons    | enable dashboard               | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:21 IST | 09 May 25 15:21 IST |
| stop      |                                | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:21 IST | 09 May 25 15:21 IST |
| start     |                                | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:21 IST | 09 May 25 15:22 IST |
| addons    | enable metrics-server          | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:22 IST | 09 May 25 15:22 IST |
| service   | --all                          | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:22 IST |                     |
| dashboard |                                | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:22 IST |                     |
| service   | kmyclassjar-service            | minikube | PARADISE\KUHAN | v1.35.0 | 09 May 25 15:48 IST | 09 May 25 15:49 IST |
|-----------|--------------------------------|----------|----------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/05/09 15:21:33
Running on machine: Paradise
Binary: Built with gc go1.23.4 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0509 15:21:33.742107    7476 out.go:345] Setting OutFile to fd 224 ...
I0509 15:21:33.743215    7476 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0509 15:21:33.743215    7476 out.go:358] Setting ErrFile to fd 4704...
I0509 15:21:33.743215    7476 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0509 15:21:33.760779    7476 out.go:352] Setting JSON to false
I0509 15:21:33.765884    7476 start.go:129] hostinfo: {"hostname":"Paradise","uptime":174019,"bootTime":1746610274,"procs":323,"os":"windows","platform":"Microsoft Windows 11 Home Single Language","platformFamily":"Standalone Workstation","platformVersion":"10.0.26100.3775 Build 26100.3775","kernelVersion":"10.0.26100.3775 Build 26100.3775","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"2ad4cf3c-4d67-47d1-a68f-c6b054c25176"}
W0509 15:21:33.765884    7476 start.go:137] gopshost.Virtualization returned error: not implemented yet
I0509 15:21:33.767589    7476 out.go:177] * minikube v1.35.0 on Microsoft Windows 11 Home Single Language 10.0.26100.3775 Build 26100.3775
I0509 15:21:33.768114    7476 notify.go:220] Checking for updates...
I0509 15:21:33.769151    7476 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0509 15:21:33.769151    7476 driver.go:394] Setting default libvirt URI to qemu:///system
I0509 15:21:33.839995    7476 docker.go:123] docker version: linux-28.0.4:Docker Desktop 4.40.0 (187762)
I0509 15:21:33.846319    7476 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0509 15:21:34.058357    7476 info.go:266] docker info: {ID:cb7af366-5608-4e2a-a572-13b28deb6463 Containers:5 ContainersRunning:0 ContainersPaused:0 ContainersStopped:5 Images:10 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:70 OomKillDisable:true NGoroutines:99 SystemTime:2025-05-09 09:51:34.042917954 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8161247232 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0509 15:21:34.060080    7476 out.go:177] * Using the docker driver based on existing profile
I0509 15:21:34.061174    7476 start.go:297] selected driver: docker
I0509 15:21:34.061174    7476 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\KUHAN:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0509 15:21:34.061174    7476 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0509 15:21:34.078370    7476 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0509 15:21:34.260777    7476 info.go:266] docker info: {ID:cb7af366-5608-4e2a-a572-13b28deb6463 Containers:5 ContainersRunning:0 ContainersPaused:0 ContainersStopped:5 Images:10 Driver:overlayfs DriverStatus:[[driver-type io.containerd.snapshotter.v1]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:70 OomKillDisable:true NGoroutines:99 SystemTime:2025-05-09 09:51:34.24677812 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:8161247232 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0509 15:21:34.324017    7476 cni.go:84] Creating CNI manager for ""
I0509 15:21:34.324017    7476 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0509 15:21:34.324586    7476 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\KUHAN:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0509 15:21:34.325117    7476 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0509 15:21:34.325637    7476 cache.go:121] Beginning downloading kic base image for docker with docker
I0509 15:21:34.326155    7476 out.go:177] * Pulling base image v0.0.46 ...
I0509 15:21:34.326155    7476 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0509 15:21:34.326155    7476 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local docker daemon
I0509 15:21:34.326732    7476 preload.go:146] Found local preload: C:\Users\KUHAN\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4
I0509 15:21:34.326732    7476 cache.go:56] Caching tarball of preloaded images
I0509 15:21:34.326732    7476 preload.go:172] Found C:\Users\KUHAN\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.32.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0509 15:21:34.326732    7476 cache.go:59] Finished verifying existence of preloaded tar for v1.32.0 on docker
I0509 15:21:34.326732    7476 profile.go:143] Saving config to C:\Users\KUHAN\.minikube\profiles\minikube\config.json ...
I0509 15:21:34.444121    7476 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 to local cache
I0509 15:21:34.444699    7476 localpath.go:146] windows sanitize: C:\Users\KUHAN\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\KUHAN\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0509 15:21:34.444699    7476 localpath.go:146] windows sanitize: C:\Users\KUHAN\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\KUHAN\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0509 15:21:34.444699    7476 image.go:65] Checking for gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory
I0509 15:21:34.444699    7476 image.go:68] Found gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 in local cache directory, skipping pull
I0509 15:21:34.444699    7476 image.go:137] gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 exists in cache, skipping pull
I0509 15:21:34.444699    7476 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 as a tarball
I0509 15:21:34.444699    7476 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from local cache
I0509 15:21:34.444699    7476 localpath.go:146] windows sanitize: C:\Users\KUHAN\.minikube\cache\kic\amd64\kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar -> C:\Users\KUHAN\.minikube\cache\kic\amd64\kicbase_v0.0.46@sha256_fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279.tar
I0509 15:21:51.307166    7476 cache.go:165] successfully loaded and using gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 from cached tarball
I0509 15:21:51.307166    7476 cache.go:227] Successfully downloaded all kic artifacts
I0509 15:21:51.308256    7476 start.go:360] acquireMachinesLock for minikube: {Name:mk24db93e267d53056315dd2e90810f114993ba3 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0509 15:21:51.308256    7476 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0509 15:21:51.308256    7476 start.go:96] Skipping create...Using existing machine configuration
I0509 15:21:51.308256    7476 fix.go:54] fixHost starting: 
I0509 15:21:51.368467    7476 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0509 15:21:51.430080    7476 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0509 15:21:51.430080    7476 fix.go:138] unexpected machine state, will restart: <nil>
I0509 15:21:51.430613    7476 out.go:177] * Restarting existing docker container for "minikube" ...
I0509 15:21:51.464099    7476 cli_runner.go:164] Run: docker start minikube
I0509 15:21:51.764113    7476 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0509 15:21:51.831237    7476 kic.go:430] container "minikube" state is running.
I0509 15:21:51.864875    7476 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0509 15:21:51.927868    7476 profile.go:143] Saving config to C:\Users\KUHAN\.minikube\profiles\minikube\config.json ...
I0509 15:21:51.930580    7476 machine.go:93] provisionDockerMachine start ...
I0509 15:21:51.957687    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 15:21:52.026477    7476 main.go:141] libmachine: Using SSH client type: native
I0509 15:21:52.027574    7476 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9a5360] 0x9a7ea0 <nil>  [] 0s} 127.0.0.1 61712 <nil> <nil>}
I0509 15:21:52.027574    7476 main.go:141] libmachine: About to run SSH command:
hostname
I0509 15:21:52.030559    7476 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0509 15:21:55.163828    7476 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0509 15:21:55.163828    7476 ubuntu.go:169] provisioning hostname "minikube"
I0509 15:21:55.195676    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 15:21:55.264048    7476 main.go:141] libmachine: Using SSH client type: native
I0509 15:21:55.264661    7476 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9a5360] 0x9a7ea0 <nil>  [] 0s} 127.0.0.1 61712 <nil> <nil>}
I0509 15:21:55.264661    7476 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0509 15:21:55.412322    7476 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0509 15:21:55.443217    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 15:21:55.513081    7476 main.go:141] libmachine: Using SSH client type: native
I0509 15:21:55.513755    7476 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9a5360] 0x9a7ea0 <nil>  [] 0s} 127.0.0.1 61712 <nil> <nil>}
I0509 15:21:55.513755    7476 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0509 15:21:55.664922    7476 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0509 15:21:55.664922    7476 ubuntu.go:175] set auth options {CertDir:C:\Users\KUHAN\.minikube CaCertPath:C:\Users\KUHAN\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\KUHAN\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\KUHAN\.minikube\machines\server.pem ServerKeyPath:C:\Users\KUHAN\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\KUHAN\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\KUHAN\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\KUHAN\.minikube}
I0509 15:21:55.664922    7476 ubuntu.go:177] setting up certificates
I0509 15:21:55.664922    7476 provision.go:84] configureAuth start
I0509 15:21:55.701284    7476 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0509 15:21:55.753559    7476 provision.go:143] copyHostCerts
I0509 15:21:55.754649    7476 exec_runner.go:144] found C:\Users\KUHAN\.minikube/ca.pem, removing ...
I0509 15:21:55.754649    7476 exec_runner.go:203] rm: C:\Users\KUHAN\.minikube\ca.pem
I0509 15:21:55.755180    7476 exec_runner.go:151] cp: C:\Users\KUHAN\.minikube\certs\ca.pem --> C:\Users\KUHAN\.minikube/ca.pem (1074 bytes)
I0509 15:21:55.757446    7476 exec_runner.go:144] found C:\Users\KUHAN\.minikube/cert.pem, removing ...
I0509 15:21:55.757446    7476 exec_runner.go:203] rm: C:\Users\KUHAN\.minikube\cert.pem
I0509 15:21:55.758021    7476 exec_runner.go:151] cp: C:\Users\KUHAN\.minikube\certs\cert.pem --> C:\Users\KUHAN\.minikube/cert.pem (1119 bytes)
I0509 15:21:55.760410    7476 exec_runner.go:144] found C:\Users\KUHAN\.minikube/key.pem, removing ...
I0509 15:21:55.760410    7476 exec_runner.go:203] rm: C:\Users\KUHAN\.minikube\key.pem
I0509 15:21:55.760954    7476 exec_runner.go:151] cp: C:\Users\KUHAN\.minikube\certs\key.pem --> C:\Users\KUHAN\.minikube/key.pem (1675 bytes)
I0509 15:21:55.762558    7476 provision.go:117] generating server cert: C:\Users\KUHAN\.minikube\machines\server.pem ca-key=C:\Users\KUHAN\.minikube\certs\ca.pem private-key=C:\Users\KUHAN\.minikube\certs\ca-key.pem org=KUHAN.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0509 15:21:56.400518    7476 provision.go:177] copyRemoteCerts
I0509 15:21:56.411173    7476 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0509 15:21:56.447315    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 15:21:56.493086    7476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61712 SSHKeyPath:C:\Users\KUHAN\.minikube\machines\minikube\id_rsa Username:docker}
I0509 15:21:56.596234    7476 ssh_runner.go:362] scp C:\Users\KUHAN\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0509 15:21:56.617137    7476 ssh_runner.go:362] scp C:\Users\KUHAN\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0509 15:21:56.636302    7476 ssh_runner.go:362] scp C:\Users\KUHAN\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0509 15:21:56.656312    7476 provision.go:87] duration metric: took 990.863ms to configureAuth
I0509 15:21:56.656312    7476 ubuntu.go:193] setting minikube options for container-runtime
I0509 15:21:56.656870    7476 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0509 15:21:56.690021    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 15:21:56.755374    7476 main.go:141] libmachine: Using SSH client type: native
I0509 15:21:56.755922    7476 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9a5360] 0x9a7ea0 <nil>  [] 0s} 127.0.0.1 61712 <nil> <nil>}
I0509 15:21:56.755922    7476 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0509 15:21:56.894687    7476 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0509 15:21:56.894687    7476 ubuntu.go:71] root file system type: overlay
I0509 15:21:56.894687    7476 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0509 15:21:56.931265    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 15:21:56.993702    7476 main.go:141] libmachine: Using SSH client type: native
I0509 15:21:56.994721    7476 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9a5360] 0x9a7ea0 <nil>  [] 0s} 127.0.0.1 61712 <nil> <nil>}
I0509 15:21:56.994721    7476 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0509 15:21:57.157133    7476 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0509 15:21:57.191241    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 15:21:57.255474    7476 main.go:141] libmachine: Using SSH client type: native
I0509 15:21:57.256167    7476 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x9a5360] 0x9a7ea0 <nil>  [] 0s} 127.0.0.1 61712 <nil> <nil>}
I0509 15:21:57.256167    7476 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0509 15:21:57.398421    7476 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0509 15:21:57.398421    7476 machine.go:96] duration metric: took 5.4678412s to provisionDockerMachine
I0509 15:21:57.398421    7476 start.go:293] postStartSetup for "minikube" (driver="docker")
I0509 15:21:57.398421    7476 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0509 15:21:57.408222    7476 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0509 15:21:57.440455    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 15:21:57.492078    7476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61712 SSHKeyPath:C:\Users\KUHAN\.minikube\machines\minikube\id_rsa Username:docker}
I0509 15:21:57.641685    7476 ssh_runner.go:195] Run: cat /etc/os-release
I0509 15:21:57.652484    7476 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0509 15:21:57.652484    7476 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0509 15:21:57.652484    7476 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0509 15:21:57.652484    7476 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0509 15:21:57.653083    7476 filesync.go:126] Scanning C:\Users\KUHAN\.minikube\addons for local assets ...
I0509 15:21:57.653606    7476 filesync.go:126] Scanning C:\Users\KUHAN\.minikube\files for local assets ...
I0509 15:21:57.653606    7476 start.go:296] duration metric: took 255.1857ms for postStartSetup
I0509 15:21:57.692751    7476 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0509 15:21:57.726727    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 15:21:57.773839    7476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61712 SSHKeyPath:C:\Users\KUHAN\.minikube\machines\minikube\id_rsa Username:docker}
I0509 15:21:57.909540    7476 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0509 15:21:57.918103    7476 fix.go:56] duration metric: took 6.6098467s for fixHost
I0509 15:21:57.918103    7476 start.go:83] releasing machines lock for "minikube", held for 6.6098467s
I0509 15:21:57.952602    7476 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0509 15:21:58.016275    7476 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0509 15:21:58.055982    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 15:21:58.058416    7476 ssh_runner.go:195] Run: cat /version.json
I0509 15:21:58.091738    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 15:21:58.104911    7476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61712 SSHKeyPath:C:\Users\KUHAN\.minikube\machines\minikube\id_rsa Username:docker}
I0509 15:21:58.139549    7476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61712 SSHKeyPath:C:\Users\KUHAN\.minikube\machines\minikube\id_rsa Username:docker}
W0509 15:21:58.213726    7476 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0509 15:21:58.286008    7476 ssh_runner.go:195] Run: systemctl --version
I0509 15:21:58.329714    7476 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0509 15:21:58.347012    7476 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0509 15:21:58.356963    7476 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
W0509 15:21:58.361303    7476 out.go:270] ! Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0509 15:21:58.361303    7476 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0509 15:21:58.367422    7476 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0509 15:21:58.375215    7476 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0509 15:21:58.375215    7476 start.go:495] detecting cgroup driver to use...
I0509 15:21:58.375215    7476 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0509 15:21:58.375785    7476 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0509 15:21:58.424462    7476 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0509 15:21:58.475933    7476 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0509 15:21:58.491386    7476 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0509 15:21:58.527305    7476 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0509 15:21:58.575308    7476 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0509 15:21:58.623677    7476 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0509 15:21:58.672393    7476 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0509 15:21:58.718839    7476 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0509 15:21:58.762418    7476 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0509 15:21:58.810038    7476 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0509 15:21:58.857041    7476 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0509 15:21:58.879608    7476 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0509 15:21:58.899196    7476 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0509 15:21:58.918597    7476 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0509 15:21:59.037283    7476 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0509 15:21:59.133876    7476 start.go:495] detecting cgroup driver to use...
I0509 15:21:59.134393    7476 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0509 15:21:59.143830    7476 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0509 15:21:59.156244    7476 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0509 15:21:59.165094    7476 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0509 15:21:59.177454    7476 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0509 15:21:59.229703    7476 ssh_runner.go:195] Run: which cri-dockerd
I0509 15:21:59.245555    7476 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0509 15:21:59.256106    7476 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0509 15:21:59.284709    7476 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0509 15:21:59.385227    7476 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0509 15:21:59.465090    7476 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0509 15:21:59.465616    7476 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0509 15:21:59.492308    7476 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0509 15:21:59.606331    7476 ssh_runner.go:195] Run: sudo systemctl restart docker
I0509 15:22:03.504081    7476 ssh_runner.go:235] Completed: sudo systemctl restart docker: (3.8977498s)
I0509 15:22:03.516994    7476 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0509 15:22:03.536277    7476 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0509 15:22:03.558761    7476 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0509 15:22:03.580859    7476 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0509 15:22:03.702000    7476 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0509 15:22:03.807950    7476 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0509 15:22:03.914502    7476 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0509 15:22:03.938782    7476 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0509 15:22:03.961879    7476 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0509 15:22:04.030608    7476 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0509 15:22:04.117349    7476 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0509 15:22:04.161725    7476 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0509 15:22:04.168031    7476 start.go:563] Will wait 60s for crictl version
I0509 15:22:04.214118    7476 ssh_runner.go:195] Run: which crictl
I0509 15:22:04.231561    7476 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0509 15:22:04.268317    7476 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  27.4.1
RuntimeApiVersion:  v1
I0509 15:22:04.300488    7476 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0509 15:22:04.359299    7476 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0509 15:22:04.394925    7476 out.go:235] * Preparing Kubernetes v1.32.0 on Docker 27.4.1 ...
I0509 15:22:04.427466    7476 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0509 15:22:04.516825    7476 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0509 15:22:04.561090    7476 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0509 15:22:04.572693    7476 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0509 15:22:04.617026    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0509 15:22:04.668686    7476 kubeadm.go:883] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\KUHAN:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0509 15:22:04.668686    7476 preload.go:131] Checking if preload exists for k8s version v1.32.0 and runtime docker
I0509 15:22:04.706673    7476 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0509 15:22:04.726020    7476 docker.go:689] Got preloaded images: -- stdout --
kuhancdac/kmyclassjar:latest
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0509 15:22:04.726020    7476 docker.go:619] Images already preloaded, skipping extraction
I0509 15:22:04.761093    7476 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0509 15:22:04.779187    7476 docker.go:689] Got preloaded images: -- stdout --
kuhancdac/kmyclassjar:latest
registry.k8s.io/kube-apiserver:v1.32.0
registry.k8s.io/kube-controller-manager:v1.32.0
registry.k8s.io/kube-scheduler:v1.32.0
registry.k8s.io/kube-proxy:v1.32.0
registry.k8s.io/etcd:3.5.16-0
registry.k8s.io/coredns/coredns:v1.11.3
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0509 15:22:04.779187    7476 cache_images.go:84] Images are preloaded, skipping loading
I0509 15:22:04.779187    7476 kubeadm.go:934] updating node { 192.168.49.2 8443 v1.32.0 docker true true} ...
I0509 15:22:04.779721    7476 kubeadm.go:946] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.32.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0509 15:22:04.813084    7476 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0509 15:22:04.874257    7476 cni.go:84] Creating CNI manager for ""
I0509 15:22:04.874257    7476 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0509 15:22:04.874257    7476 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0509 15:22:04.874257    7476 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.32.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0509 15:22:04.874257    7476 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.49.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.32.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0509 15:22:04.884646    7476 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.32.0
I0509 15:22:04.894690    7476 binaries.go:44] Found k8s binaries, skipping transfer
I0509 15:22:04.905505    7476 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0509 15:22:04.913760    7476 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0509 15:22:04.928700    7476 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0509 15:22:04.945485    7476 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0509 15:22:05.007243    7476 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0509 15:22:05.013969    7476 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0509 15:22:05.033408    7476 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0509 15:22:05.125829    7476 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0509 15:22:05.139442    7476 certs.go:68] Setting up C:\Users\KUHAN\.minikube\profiles\minikube for IP: 192.168.49.2
I0509 15:22:05.139442    7476 certs.go:194] generating shared ca certs ...
I0509 15:22:05.140149    7476 certs.go:226] acquiring lock for ca certs: {Name:mkd38fd11647188791be9d05f171024a8bdc4122 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 15:22:05.140678    7476 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\KUHAN\.minikube\ca.key
I0509 15:22:05.140678    7476 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\KUHAN\.minikube\proxy-client-ca.key
I0509 15:22:05.141275    7476 certs.go:256] generating profile certs ...
I0509 15:22:05.141895    7476 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\KUHAN\.minikube\profiles\minikube\client.key
I0509 15:22:05.142503    7476 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\KUHAN\.minikube\profiles\minikube\apiserver.key.7fb57e3c
I0509 15:22:05.143111    7476 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\KUHAN\.minikube\profiles\minikube\proxy-client.key
I0509 15:22:05.144928    7476 certs.go:484] found cert: C:\Users\KUHAN\.minikube\certs\ca-key.pem (1675 bytes)
I0509 15:22:05.144928    7476 certs.go:484] found cert: C:\Users\KUHAN\.minikube\certs\ca.pem (1074 bytes)
I0509 15:22:05.145580    7476 certs.go:484] found cert: C:\Users\KUHAN\.minikube\certs\cert.pem (1119 bytes)
I0509 15:22:05.145580    7476 certs.go:484] found cert: C:\Users\KUHAN\.minikube\certs\key.pem (1675 bytes)
I0509 15:22:05.147330    7476 ssh_runner.go:362] scp C:\Users\KUHAN\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0509 15:22:05.169465    7476 ssh_runner.go:362] scp C:\Users\KUHAN\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0509 15:22:05.190608    7476 ssh_runner.go:362] scp C:\Users\KUHAN\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0509 15:22:05.212082    7476 ssh_runner.go:362] scp C:\Users\KUHAN\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0509 15:22:05.239279    7476 ssh_runner.go:362] scp C:\Users\KUHAN\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0509 15:22:05.318927    7476 ssh_runner.go:362] scp C:\Users\KUHAN\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0509 15:22:05.415891    7476 ssh_runner.go:362] scp C:\Users\KUHAN\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0509 15:22:05.437293    7476 ssh_runner.go:362] scp C:\Users\KUHAN\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0509 15:22:05.506327    7476 ssh_runner.go:362] scp C:\Users\KUHAN\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0509 15:22:05.527157    7476 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0509 15:22:05.592933    7476 ssh_runner.go:195] Run: openssl version
I0509 15:22:05.606913    7476 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0509 15:22:05.662501    7476 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0509 15:22:05.667880    7476 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May  9 03:47 /usr/share/ca-certificates/minikubeCA.pem
I0509 15:22:05.711071    7476 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0509 15:22:05.732244    7476 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0509 15:22:05.775129    7476 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0509 15:22:05.818109    7476 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0509 15:22:05.876552    7476 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0509 15:22:05.937530    7476 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0509 15:22:06.050258    7476 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0509 15:22:06.151605    7476 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0509 15:22:06.237953    7476 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0509 15:22:06.248152    7476 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.46@sha256:fd2d445ddcc33ebc5c6b68a17e6219ea207ce63c005095ea1525296da2d1a279 Memory:4000 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.32.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[dashboard:true default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\KUHAN:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0509 15:22:06.280493    7476 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0509 15:22:06.315523    7476 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0509 15:22:06.395293    7476 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0509 15:22:06.395293    7476 kubeadm.go:593] restartPrimaryControlPlane start ...
I0509 15:22:06.403104    7476 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0509 15:22:06.414847    7476 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0509 15:22:06.447035    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0509 15:22:06.501524    7476 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in C:\Users\KUHAN\.kube\config
I0509 15:22:06.502050    7476 kubeconfig.go:62] C:\Users\KUHAN\.kube\config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0509 15:22:06.503175    7476 lock.go:35] WriteFile acquiring C:\Users\KUHAN\.kube\config: {Name:mk36de63851ade61263d527f3e0935e7ad788492 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 15:22:06.540667    7476 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0509 15:22:06.615344    7476 kubeadm.go:630] The running cluster does not require reconfiguration: 127.0.0.1
I0509 15:22:06.616052    7476 kubeadm.go:597] duration metric: took 220.7592ms to restartPrimaryControlPlane
I0509 15:22:06.616052    7476 kubeadm.go:394] duration metric: took 367.9008ms to StartCluster
I0509 15:22:06.616052    7476 settings.go:142] acquiring lock: {Name:mk46b2e5c3c41f7ede98e72fa9e6e4ca2c68b029 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 15:22:06.616052    7476 settings.go:150] Updating kubeconfig:  C:\Users\KUHAN\.kube\config
I0509 15:22:06.618867    7476 lock.go:35] WriteFile acquiring C:\Users\KUHAN\.kube\config: {Name:mk36de63851ade61263d527f3e0935e7ad788492 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0509 15:22:06.619576    7476 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.32.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0509 15:22:06.620332    7476 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:true default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0509 15:22:06.620332    7476 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0509 15:22:06.620332    7476 addons.go:238] Setting addon storage-provisioner=true in "minikube"
W0509 15:22:06.620332    7476 addons.go:247] addon storage-provisioner should already be in state true
I0509 15:22:06.620332    7476 addons.go:69] Setting dashboard=true in profile "minikube"
I0509 15:22:06.620332    7476 addons.go:238] Setting addon dashboard=true in "minikube"
W0509 15:22:06.620332    7476 addons.go:247] addon dashboard should already be in state true
I0509 15:22:06.620332    7476 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0509 15:22:06.620915    7476 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.32.0
I0509 15:22:06.620915    7476 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0509 15:22:06.620915    7476 host.go:66] Checking if "minikube" exists ...
I0509 15:22:06.620915    7476 out.go:177] * Verifying Kubernetes components...
I0509 15:22:06.621511    7476 host.go:66] Checking if "minikube" exists ...
I0509 15:22:06.635026    7476 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0509 15:22:06.708087    7476 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0509 15:22:06.708087    7476 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0509 15:22:06.712404    7476 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0509 15:22:06.768046    7476 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0509 15:22:06.768566    7476 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0509 15:22:06.768566    7476 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0509 15:22:06.769108    7476 out.go:177]   - Using image docker.io/kubernetesui/dashboard:v2.7.0
I0509 15:22:06.769653    7476 out.go:177]   - Using image docker.io/kubernetesui/metrics-scraper:v1.0.8
I0509 15:22:06.770203    7476 addons.go:435] installing /etc/kubernetes/addons/dashboard-ns.yaml
I0509 15:22:06.770203    7476 ssh_runner.go:362] scp dashboard/dashboard-ns.yaml --> /etc/kubernetes/addons/dashboard-ns.yaml (759 bytes)
I0509 15:22:06.775433    7476 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0509 15:22:06.775433    7476 addons.go:247] addon default-storageclass should already be in state true
I0509 15:22:06.776011    7476 host.go:66] Checking if "minikube" exists ...
I0509 15:22:06.812295    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 15:22:06.816562    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 15:22:06.854951    7476 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0509 15:22:06.869870    7476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61712 SSHKeyPath:C:\Users\KUHAN\.minikube\machines\minikube\id_rsa Username:docker}
I0509 15:22:06.870436    7476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61712 SSHKeyPath:C:\Users\KUHAN\.minikube\machines\minikube\id_rsa Username:docker}
I0509 15:22:06.917621    7476 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0509 15:22:06.917621    7476 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0509 15:22:06.949327    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0509 15:22:07.007374    7476 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61712 SSHKeyPath:C:\Users\KUHAN\.minikube\machines\minikube\id_rsa Username:docker}
I0509 15:22:07.111599    7476 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0509 15:22:07.208657    7476 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0509 15:22:07.296539    7476 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrole.yaml
I0509 15:22:07.296539    7476 ssh_runner.go:362] scp dashboard/dashboard-clusterrole.yaml --> /etc/kubernetes/addons/dashboard-clusterrole.yaml (1001 bytes)
I0509 15:22:07.330739    7476 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0509 15:22:07.379870    7476 api_server.go:52] waiting for apiserver process to appear ...
I0509 15:22:07.390198    7476 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0509 15:22:07.493250    7476 addons.go:435] installing /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml
I0509 15:22:07.493250    7476 ssh_runner.go:362] scp dashboard/dashboard-clusterrolebinding.yaml --> /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml (1018 bytes)
I0509 15:22:07.506243    7476 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0509 15:22:07.518631    7476 addons.go:435] installing /etc/kubernetes/addons/dashboard-configmap.yaml
I0509 15:22:07.518631    7476 ssh_runner.go:362] scp dashboard/dashboard-configmap.yaml --> /etc/kubernetes/addons/dashboard-configmap.yaml (837 bytes)
I0509 15:22:07.606597    7476 addons.go:435] installing /etc/kubernetes/addons/dashboard-dp.yaml
I0509 15:22:07.606597    7476 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/dashboard-dp.yaml (4288 bytes)
I0509 15:22:07.703299    7476 addons.go:435] installing /etc/kubernetes/addons/dashboard-role.yaml
I0509 15:22:07.703299    7476 ssh_runner.go:362] scp dashboard/dashboard-role.yaml --> /etc/kubernetes/addons/dashboard-role.yaml (1724 bytes)
I0509 15:22:07.795722    7476 addons.go:435] installing /etc/kubernetes/addons/dashboard-rolebinding.yaml
I0509 15:22:07.795722    7476 ssh_runner.go:362] scp dashboard/dashboard-rolebinding.yaml --> /etc/kubernetes/addons/dashboard-rolebinding.yaml (1046 bytes)
I0509 15:22:07.827338    7476 addons.go:435] installing /etc/kubernetes/addons/dashboard-sa.yaml
I0509 15:22:07.827338    7476 ssh_runner.go:362] scp dashboard/dashboard-sa.yaml --> /etc/kubernetes/addons/dashboard-sa.yaml (837 bytes)
I0509 15:22:07.908720    7476 addons.go:435] installing /etc/kubernetes/addons/dashboard-secret.yaml
I0509 15:22:07.908720    7476 ssh_runner.go:362] scp dashboard/dashboard-secret.yaml --> /etc/kubernetes/addons/dashboard-secret.yaml (1389 bytes)
I0509 15:22:07.989717    7476 addons.go:435] installing /etc/kubernetes/addons/dashboard-svc.yaml
I0509 15:22:07.989717    7476 ssh_runner.go:362] scp dashboard/dashboard-svc.yaml --> /etc/kubernetes/addons/dashboard-svc.yaml (1294 bytes)
I0509 15:22:08.025957    7476 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml
I0509 15:22:13.203652    7476 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (5.993982s)
I0509 15:22:13.203652    7476 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (5.8124415s)
I0509 15:22:13.203652    7476 api_server.go:72] duration metric: took 6.5840753s to wait for apiserver process to appear ...
I0509 15:22:13.203652    7476 api_server.go:88] waiting for apiserver healthz status ...
I0509 15:22:13.203652    7476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61716/healthz ...
I0509 15:22:13.203652    7476 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (5.697409s)
I0509 15:22:13.296631    7476 api_server.go:279] https://127.0.0.1:61716/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0509 15:22:13.296631    7476 api_server.go:103] status: https://127.0.0.1:61716/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0509 15:22:13.704783    7476 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61716/healthz ...
I0509 15:22:13.805237    7476 api_server.go:279] https://127.0.0.1:61716/healthz returned 200:
ok
I0509 15:22:13.895077    7476 api_server.go:141] control plane version: v1.32.0
I0509 15:22:13.895077    7476 api_server.go:131] duration metric: took 691.4256ms to wait for apiserver health ...
I0509 15:22:13.895077    7476 system_pods.go:43] waiting for kube-system pods to appear ...
I0509 15:22:13.910353    7476 system_pods.go:59] 7 kube-system pods found
I0509 15:22:13.910353    7476 system_pods.go:61] "coredns-668d6bf9bc-8gldt" [71650896-442f-45f0-8d15-a45ccaf929fc] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0509 15:22:13.910353    7476 system_pods.go:61] "etcd-minikube" [060adc27-1639-4a23-9f48-fd904dc156b4] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0509 15:22:13.910353    7476 system_pods.go:61] "kube-apiserver-minikube" [8c038f0e-16b0-484a-9cb6-8d67286a5884] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0509 15:22:13.910353    7476 system_pods.go:61] "kube-controller-manager-minikube" [700e2069-ef1d-4a3e-9af7-d88e346476a5] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0509 15:22:13.910353    7476 system_pods.go:61] "kube-proxy-rrlhn" [af5e1b7c-96ba-44e5-be82-2a10f72c16cc] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0509 15:22:13.910353    7476 system_pods.go:61] "kube-scheduler-minikube" [85f7b023-06ce-41ff-9139-97fc35951d55] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0509 15:22:13.910353    7476 system_pods.go:61] "storage-provisioner" [056a0739-be7c-48d1-8940-ca34358e73f6] Running
I0509 15:22:13.910353    7476 system_pods.go:74] duration metric: took 15.2754ms to wait for pod list to return data ...
I0509 15:22:13.910353    7476 kubeadm.go:582] duration metric: took 7.2907763s to wait for: map[apiserver:true system_pods:true]
I0509 15:22:13.910353    7476 node_conditions.go:102] verifying NodePressure condition ...
I0509 15:22:13.993267    7476 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0509 15:22:13.993267    7476 node_conditions.go:123] node cpu capacity is 12
I0509 15:22:13.993267    7476 node_conditions.go:105] duration metric: took 82.9141ms to run NodePressure ...
I0509 15:22:13.993267    7476 start.go:241] waiting for startup goroutines ...
I0509 15:22:15.963968    7476 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.32.0/kubectl apply -f /etc/kubernetes/addons/dashboard-ns.yaml -f /etc/kubernetes/addons/dashboard-clusterrole.yaml -f /etc/kubernetes/addons/dashboard-clusterrolebinding.yaml -f /etc/kubernetes/addons/dashboard-configmap.yaml -f /etc/kubernetes/addons/dashboard-dp.yaml -f /etc/kubernetes/addons/dashboard-role.yaml -f /etc/kubernetes/addons/dashboard-rolebinding.yaml -f /etc/kubernetes/addons/dashboard-sa.yaml -f /etc/kubernetes/addons/dashboard-secret.yaml -f /etc/kubernetes/addons/dashboard-svc.yaml: (7.9369904s)
I0509 15:22:15.965593    7476 out.go:177] * Some dashboard features require the metrics-server addon. To enable all features please run:

	minikube addons enable metrics-server

I0509 15:22:15.966134    7476 out.go:177] * Enabled addons: storage-provisioner, default-storageclass, dashboard
I0509 15:22:15.967231    7476 addons.go:514] duration metric: took 9.3476544s for enable addons: enabled=[storage-provisioner default-storageclass dashboard]
I0509 15:22:15.967231    7476 start.go:246] waiting for cluster config update ...
I0509 15:22:15.967231    7476 start.go:255] writing updated cluster config ...
I0509 15:22:16.014821    7476 ssh_runner.go:195] Run: rm -f paused
I0509 15:22:16.141549    7476 start.go:600] kubectl: 1.32.2, cluster: 1.32.0 (minor skew: 0)
I0509 15:22:16.142057    7476 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 09 09:52:04 minikube cri-dockerd[1398]: time="2025-05-09T09:52:04Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
May 09 09:52:04 minikube cri-dockerd[1398]: time="2025-05-09T09:52:04Z" level=info msg="Start docker client with request timeout 0s"
May 09 09:52:04 minikube cri-dockerd[1398]: time="2025-05-09T09:52:04Z" level=info msg="Hairpin mode is set to hairpin-veth"
May 09 09:52:04 minikube cri-dockerd[1398]: time="2025-05-09T09:52:04Z" level=info msg="Loaded network plugin cni"
May 09 09:52:04 minikube cri-dockerd[1398]: time="2025-05-09T09:52:04Z" level=info msg="Docker cri networking managed by network plugin cni"
May 09 09:52:04 minikube cri-dockerd[1398]: time="2025-05-09T09:52:04Z" level=info msg="Setting cgroupDriver cgroupfs"
May 09 09:52:04 minikube cri-dockerd[1398]: time="2025-05-09T09:52:04Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
May 09 09:52:04 minikube cri-dockerd[1398]: time="2025-05-09T09:52:04Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
May 09 09:52:04 minikube cri-dockerd[1398]: time="2025-05-09T09:52:04Z" level=info msg="Start cri-dockerd grpc backend"
May 09 09:52:04 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
May 09 09:52:05 minikube cri-dockerd[1398]: time="2025-05-09T09:52:05Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kubernetes-dashboard-7779f9b69b-jpjn7_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"5c4789a720751cbf384001221b3853b35adbc495a408daa9bb9e9b774917e826\""
May 09 09:52:05 minikube cri-dockerd[1398]: time="2025-05-09T09:52:05Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"kmyclassjar-deployment-5c86db7f6b-8x62f_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"0393b05f9de1ab141c15a0e504070b0a4c34b5764dc7810cfc5e7ba744b69d1c\""
May 09 09:52:05 minikube cri-dockerd[1398]: time="2025-05-09T09:52:05Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-668d6bf9bc-8gldt_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"e187d2636c60e6514967610986df88020490059b9e5010d025a67b5d15537363\""
May 09 09:52:05 minikube cri-dockerd[1398]: time="2025-05-09T09:52:05Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"dashboard-metrics-scraper-5d59dccf9b-78djm_kubernetes-dashboard\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9e30ff23560d56eead6e53d895dd69dce1116be2fc219ba953501c34f2c3395d\""
May 09 09:52:06 minikube cri-dockerd[1398]: time="2025-05-09T09:52:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/a78c5576dae7e85be6167ca114875778a7f26e7f62c82c69775c3e533423841c/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 09 09:52:06 minikube cri-dockerd[1398]: time="2025-05-09T09:52:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/da0eb9d959ab75582db7df2ac10cfbd5e21dc02bb7199a27e77dd9e487a17b6b/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 09 09:52:06 minikube cri-dockerd[1398]: time="2025-05-09T09:52:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/325afb5251fa0d2ac0415c66919bb652030a0d1a3741d03617eddbacbb5a6567/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 09 09:52:06 minikube cri-dockerd[1398]: time="2025-05-09T09:52:06Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/3f3685a6797b42d3acd356781c11b995ee6e2a8993d4dd5690feded455701d55/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 09 09:52:10 minikube cri-dockerd[1398]: time="2025-05-09T09:52:10Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
May 09 09:52:12 minikube cri-dockerd[1398]: time="2025-05-09T09:52:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/efa8ed2f2a76db8b85cf825162f09c627622b0c0646e443fd90e7b4bba99df1e/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 09 09:52:12 minikube cri-dockerd[1398]: time="2025-05-09T09:52:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4a0b6911a243d6cdddd02dec4bdfcce9dd4c5cc28dff16ba83a19e12a4d609e9/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 09 09:52:13 minikube cri-dockerd[1398]: time="2025-05-09T09:52:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/6de65f619149453aa5c9b91f4b58bce546f0d2657d784a939b0dd8673dd9a04b/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 09 09:52:13 minikube cri-dockerd[1398]: time="2025-05-09T09:52:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/bf3293b62dd15ef7640b96e16f8d0d47170224a24e8c1271b9d35af46375e5d0/resolv.conf as [nameserver 10.96.0.10 search kubernetes-dashboard.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 09 09:52:13 minikube cri-dockerd[1398]: time="2025-05-09T09:52:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5d683c4a474f570e37f432c77c2b48f40867687c496b07a690884d3b25470187/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 09 09:52:13 minikube cri-dockerd[1398]: time="2025-05-09T09:52:13Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/2b746288b4d91eeae9dc29f5c17a44a8eb3802e78e3c6dcb8aed6190f20fa925/resolv.conf as [nameserver 192.168.65.254 options ndots:0]"
May 09 09:52:14 minikube dockerd[1111]: time="2025-05-09T09:52:14.792788643Z" level=warning msg="reference for unknown type: " digest="sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93" remote="docker.io/kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
May 09 09:52:19 minikube cri-dockerd[1398]: time="2025-05-09T09:52:19Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/07c5b68b54350646c030fedff479730c31b9578f53667587fd23f61be54b1ba1/resolv.conf as [nameserver 10.96.0.10 search kube-system.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
May 09 09:52:25 minikube cri-dockerd[1398]: time="2025-05-09T09:52:25Z" level=info msg="Stop pulling image docker.io/kubernetesui/dashboard:v2.7.0@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93: Status: Downloaded newer image for kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93"
May 09 09:52:34 minikube dockerd[1111]: time="2025-05-09T09:52:34.465905853Z" level=info msg="ignoring event" container=b2b30b78d55cebba26f80cba10641f15b2533f3e642385313a3bdd5360516a21 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 09 09:52:39 minikube cri-dockerd[1398]: time="2025-05-09T09:52:39Z" level=info msg="Pulling image kuhancdac/kmyclassjar:latest: 38a980f2cc8a: Extracting [==========>                                        ]   8.52MB/42.11MB"
May 09 09:52:48 minikube cri-dockerd[1398]: time="2025-05-09T09:52:48Z" level=info msg="Stop pulling image kuhancdac/kmyclassjar:latest: Status: Downloaded newer image for kuhancdac/kmyclassjar:latest"
May 09 09:52:48 minikube dockerd[1111]: time="2025-05-09T09:52:48.524447691Z" level=info msg="ignoring event" container=bf8ea485a492fa9086e31c1b7d2d1889913aed4b0f6020cda964f489928d93e9 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 09 09:52:49 minikube dockerd[1111]: time="2025-05-09T09:52:49.054071263Z" level=warning msg="reference for unknown type: " digest="sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c" remote="docker.io/kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
May 09 09:52:54 minikube cri-dockerd[1398]: time="2025-05-09T09:52:54Z" level=info msg="Stop pulling image docker.io/kubernetesui/metrics-scraper:v1.0.8@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c: Status: Downloaded newer image for kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c"
May 09 09:52:54 minikube dockerd[1111]: time="2025-05-09T09:52:54.424881241Z" level=warning msg="reference for unknown type: " digest="sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9" remote="registry.k8s.io/metrics-server/metrics-server@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9"
May 09 09:53:00 minikube cri-dockerd[1398]: time="2025-05-09T09:53:00Z" level=info msg="Stop pulling image registry.k8s.io/metrics-server/metrics-server:v0.7.2@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9: Status: Downloaded newer image for registry.k8s.io/metrics-server/metrics-server@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9"
May 09 09:53:03 minikube cri-dockerd[1398]: time="2025-05-09T09:53:03Z" level=info msg="Stop pulling image kuhancdac/kmyclassjar:latest: Status: Image is up to date for kuhancdac/kmyclassjar:latest"
May 09 09:53:03 minikube dockerd[1111]: time="2025-05-09T09:53:03.391549218Z" level=info msg="ignoring event" container=60f6673be46365bbf1c32fc004b45e31f7b62a7d0c61ad4c033f37292b7596b4 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 09 09:53:21 minikube cri-dockerd[1398]: time="2025-05-09T09:53:21Z" level=info msg="Stop pulling image kuhancdac/kmyclassjar:latest: Status: Image is up to date for kuhancdac/kmyclassjar:latest"
May 09 09:53:22 minikube dockerd[1111]: time="2025-05-09T09:53:22.018753448Z" level=info msg="ignoring event" container=440f4ce071279180940328047cfe3866474f4b47e32d8faa4ed14c0b7642dc51 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 09 09:53:52 minikube cri-dockerd[1398]: time="2025-05-09T09:53:52Z" level=info msg="Stop pulling image kuhancdac/kmyclassjar:latest: Status: Image is up to date for kuhancdac/kmyclassjar:latest"
May 09 09:53:52 minikube dockerd[1111]: time="2025-05-09T09:53:52.866974577Z" level=info msg="ignoring event" container=53be6013d392d89fc1802d97f26524cfa60b4aab9110a573a2a9d27b21a37c2b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 09 09:54:40 minikube cri-dockerd[1398]: time="2025-05-09T09:54:40Z" level=info msg="Stop pulling image kuhancdac/kmyclassjar:latest: Status: Image is up to date for kuhancdac/kmyclassjar:latest"
May 09 09:54:40 minikube dockerd[1111]: time="2025-05-09T09:54:40.931693696Z" level=info msg="ignoring event" container=000584a7be25df82f66153472d344cd8253cc1f44a0466bae41dfefa471a1828 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 09 09:56:12 minikube cri-dockerd[1398]: time="2025-05-09T09:56:12Z" level=info msg="Stop pulling image kuhancdac/kmyclassjar:latest: Status: Image is up to date for kuhancdac/kmyclassjar:latest"
May 09 09:56:12 minikube dockerd[1111]: time="2025-05-09T09:56:12.955465494Z" level=info msg="ignoring event" container=f0a1419daf5097c211d7e121bfda597699ad97fb77af44cd56f1dd75682f4872 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 09 09:59:03 minikube cri-dockerd[1398]: time="2025-05-09T09:59:03Z" level=info msg="Stop pulling image kuhancdac/kmyclassjar:latest: Status: Image is up to date for kuhancdac/kmyclassjar:latest"
May 09 09:59:03 minikube dockerd[1111]: time="2025-05-09T09:59:03.875072707Z" level=info msg="ignoring event" container=1a205c8710ec1a5230d1b0ffd470bd346d5c1cf40db0b203fd68405c4433d793 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 09 10:04:18 minikube cri-dockerd[1398]: time="2025-05-09T10:04:18Z" level=info msg="Stop pulling image kuhancdac/kmyclassjar:latest: Status: Image is up to date for kuhancdac/kmyclassjar:latest"
May 09 10:04:18 minikube dockerd[1111]: time="2025-05-09T10:04:18.872579372Z" level=info msg="ignoring event" container=bfe16c45b723bd078163e4c6c7dd770beda2a8ad9ccf33085439298ff5e69c3c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 09 10:09:31 minikube cri-dockerd[1398]: time="2025-05-09T10:09:31Z" level=info msg="Stop pulling image kuhancdac/kmyclassjar:latest: Status: Image is up to date for kuhancdac/kmyclassjar:latest"
May 09 10:09:31 minikube dockerd[1111]: time="2025-05-09T10:09:31.818445674Z" level=info msg="ignoring event" container=845dcef7f53767898973abea13adc770754d2757e02cd499fd84d00676ec4ac3 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 09 10:14:36 minikube cri-dockerd[1398]: time="2025-05-09T10:14:36Z" level=info msg="Stop pulling image kuhancdac/kmyclassjar:latest: Status: Image is up to date for kuhancdac/kmyclassjar:latest"
May 09 10:14:36 minikube dockerd[1111]: time="2025-05-09T10:14:36.805747224Z" level=info msg="ignoring event" container=da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 09 10:14:37 minikube cri-dockerd[1398]: time="2025-05-09T10:14:37Z" level=error msg="error getting RW layer size for container ID '845dcef7f53767898973abea13adc770754d2757e02cd499fd84d00676ec4ac3': Error response from daemon: No such container: 845dcef7f53767898973abea13adc770754d2757e02cd499fd84d00676ec4ac3"
May 09 10:14:37 minikube cri-dockerd[1398]: time="2025-05-09T10:14:37Z" level=error msg="Set backoffDuration to : 1m0s for container ID '845dcef7f53767898973abea13adc770754d2757e02cd499fd84d00676ec4ac3'"
May 09 10:19:43 minikube cri-dockerd[1398]: time="2025-05-09T10:19:43Z" level=info msg="Stop pulling image kuhancdac/kmyclassjar:latest: Status: Image is up to date for kuhancdac/kmyclassjar:latest"
May 09 10:19:43 minikube dockerd[1111]: time="2025-05-09T10:19:43.816547084Z" level=info msg="ignoring event" container=d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
May 09 10:21:38 minikube cri-dockerd[1398]: E0509 10:21:38.884917    1398 httpstream.go:257] error forwarding port 80 to pod 5d683c4a474f570e37f432c77c2b48f40867687c496b07a690884d3b25470187, uid : exit status 1: 2025/05/09 10:21:38 socat[10722] E connect(5, AF=2 127.0.0.1:80, 16): Connection refused
May 09 10:21:38 minikube cri-dockerd[1398]: E0509 10:21:38.884917    1398 httpstream.go:257] error forwarding port 80 to pod 5d683c4a474f570e37f432c77c2b48f40867687c496b07a690884d3b25470187, uid : exit status 1: 2025/05/09 10:21:38 socat[10721] E connect(5, AF=2 127.0.0.1:80, 16): Connection refused


==> container status <==
CONTAINER           IMAGE                                                                                                                   CREATED             STATE               NAME                        ATTEMPT             POD ID              POD
d7af487ae425f       kuhancdac/kmyclassjar@sha256:72d7bd8646cdbe275a71376310619de1d2c9267f5a8f82a5905b54d827e1cf10                           3 minutes ago       Exited              kmyclassjar                 10                  5d683c4a474f5       kmyclassjar-deployment-5c86db7f6b-8x62f
e7849110c3c07       registry.k8s.io/metrics-server/metrics-server@sha256:ffcb2bf004d6aa0a17d90e0247cf94f2865c8901dcab4427034c341951c239f9   30 minutes ago      Running             metrics-server              0                   07c5b68b54350       metrics-server-7fbb699795-8jpmb
fdc688213348c       kubernetesui/metrics-scraper@sha256:76049887f07a0476dc93efc2d3569b9529bf982b22d29f356092ce206e98765c                    30 minutes ago      Running             dashboard-metrics-scraper   0                   bf3293b62dd15       dashboard-metrics-scraper-5d59dccf9b-78djm
edd4e3e79a1d3       6e38f40d628db                                                                                                           30 minutes ago      Running             storage-provisioner         3                   efa8ed2f2a76d       storage-provisioner
8aa852385f553       kubernetesui/dashboard@sha256:2e500d29e9d5f4a086b908eb8dfe7ecac57d2ab09d65b24f588b1d449841ef93                          30 minutes ago      Running             kubernetes-dashboard        0                   6de65f6191494       kubernetes-dashboard-7779f9b69b-jpjn7
aa05371fae219       c69fa2e9cbf5f                                                                                                           31 minutes ago      Running             coredns                     1                   2b746288b4d91       coredns-668d6bf9bc-8gldt
fad65ca5223e0       040f9f8aac8cd                                                                                                           31 minutes ago      Running             kube-proxy                  1                   4a0b6911a243d       kube-proxy-rrlhn
b2b30b78d55ce       6e38f40d628db                                                                                                           31 minutes ago      Exited              storage-provisioner         2                   efa8ed2f2a76d       storage-provisioner
3d1be100257b7       a389e107f4ff1                                                                                                           31 minutes ago      Running             kube-scheduler              1                   a78c5576dae7e       kube-scheduler-minikube
06b0fe78c8a23       a9e7e6b294baf                                                                                                           31 minutes ago      Running             etcd                        1                   3f3685a6797b4       etcd-minikube
ebad1b6703cce       c2e17b8d0f4a3                                                                                                           31 minutes ago      Running             kube-apiserver              1                   da0eb9d959ab7       kube-apiserver-minikube
b1a7ee98ce24b       8cab3d2a8bd0f                                                                                                           31 minutes ago      Running             kube-controller-manager     1                   325afb5251fa0       kube-controller-manager-minikube
f2ffaf221d0a6       c69fa2e9cbf5f                                                                                                           32 minutes ago      Exited              coredns                     0                   e187d2636c60e       coredns-668d6bf9bc-8gldt
d4febcd81c679       040f9f8aac8cd                                                                                                           32 minutes ago      Exited              kube-proxy                  0                   cdb22f011fdaa       kube-proxy-rrlhn
7101afa9e577c       a9e7e6b294baf                                                                                                           32 minutes ago      Exited              etcd                        0                   35f9f0e799c3c       etcd-minikube
84c7380bc1deb       c2e17b8d0f4a3                                                                                                           32 minutes ago      Exited              kube-apiserver              0                   f1770a0b6661b       kube-apiserver-minikube
5a6d22d5e9079       a389e107f4ff1                                                                                                           32 minutes ago      Exited              kube-scheduler              0                   1f620881ad14a       kube-scheduler-minikube
836ce83840643       8cab3d2a8bd0f                                                                                                           32 minutes ago      Exited              kube-controller-manager     0                   cc5e06d119b04       kube-controller-manager-minikube


==> coredns [aa05371fae21] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:49227 - 15698 "HINFO IN 3761020876024765411.4648950377410292524. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.099489484s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[82691270]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (09-May-2025 09:52:14.623) (total time: 21049ms):
Trace[82691270]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21049ms (09:52:35.670)
Trace[82691270]: [21.049978145s] [21.049978145s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[969966213]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (09-May-2025 09:52:14.623) (total time: 21050ms):
Trace[969966213]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21049ms (09:52:35.670)
Trace[969966213]: [21.050166344s] [21.050166344s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1972356444]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (09-May-2025 09:52:14.623) (total time: 21050ms):
Trace[1972356444]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21049ms (09:52:35.670)
Trace[1972356444]: [21.050264767s] [21.050264767s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused


==> coredns [f2ffaf221d0a] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.11.3
linux/amd64, go1.21.11, a6338e9
[INFO] 127.0.0.1:57952 - 35099 "HINFO IN 4898656315307097152.8621563860004527558. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.046316039s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[301559570]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (09-May-2025 09:50:58.225) (total time: 21066ms):
Trace[301559570]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21035ms (09:51:19.261)
Trace[301559570]: [21.066877592s] [21.066877592s] END
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1063405822]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (09-May-2025 09:50:58.225) (total time: 21067ms):
Trace[1063405822]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21036ms (09:51:19.261)
Trace[1063405822]: [21.067156185s] [21.067156185s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] plugin/kubernetes: Trace[1381951437]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229 (09-May-2025 09:50:58.225) (total time: 21067ms):
Trace[1381951437]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused 21035ms (09:51:19.261)
Trace[1381951437]: [21.067426412s] [21.067426412s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.29.3/tools/cache/reflector.go:229: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: connect: connection refused
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=dd5d320e41b5451cdf3c01891bc4e13d189586ed-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_05_09T15_20_52_0700
                    minikube.k8s.io/version=v1.35.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Fri, 09 May 2025 09:50:49 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Fri, 09 May 2025 10:23:07 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Fri, 09 May 2025 10:20:46 +0000   Fri, 09 May 2025 09:50:47 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Fri, 09 May 2025 10:20:46 +0000   Fri, 09 May 2025 09:50:47 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Fri, 09 May 2025 10:20:46 +0000   Fri, 09 May 2025 09:50:47 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Fri, 09 May 2025 10:20:46 +0000   Fri, 09 May 2025 09:50:49 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7969968Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             7969968Ki
  pods:               110
System Info:
  Machine ID:                 1a97227e367343c9867ae0972eb0246e
  System UUID:                1a97227e367343c9867ae0972eb0246e
  Boot ID:                    fb12d98f-a4ab-46b7-aed0-289af614401d
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://27.4.1
  Kubelet Version:            v1.32.0
  Kube-Proxy Version:         v1.32.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                          CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                          ------------  ----------  ---------------  -------------  ---
  default                     kmyclassjar-deployment-5c86db7f6b-8x62f       0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m
  kube-system                 coredns-668d6bf9bc-8gldt                      100m (0%)     0 (0%)      70Mi (0%)        170Mi (2%)     32m
  kube-system                 etcd-minikube                                 100m (0%)     0 (0%)      100Mi (1%)       0 (0%)         32m
  kube-system                 kube-apiserver-minikube                       250m (2%)     0 (0%)      0 (0%)           0 (0%)         32m
  kube-system                 kube-controller-manager-minikube              200m (1%)     0 (0%)      0 (0%)           0 (0%)         32m
  kube-system                 kube-proxy-rrlhn                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         32m
  kube-system                 kube-scheduler-minikube                       100m (0%)     0 (0%)      0 (0%)           0 (0%)         32m
  kube-system                 metrics-server-7fbb699795-8jpmb               100m (0%)     0 (0%)      200Mi (2%)       0 (0%)         30m
  kube-system                 storage-provisioner                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         32m
  kubernetes-dashboard        dashboard-metrics-scraper-5d59dccf9b-78djm    0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m
  kubernetes-dashboard        kubernetes-dashboard-7779f9b69b-jpjn7         0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (7%)   0 (0%)
  memory             370Mi (4%)  170Mi (2%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           32m                kube-proxy       
  Normal   Starting                           30m                kube-proxy       
  Warning  PossibleMemoryBackedVolumesOnDisk  32m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           32m                kubelet          Starting kubelet.
  Warning  CgroupV1                           32m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientPID               32m                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasSufficientMemory            32m                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              32m                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeAllocatableEnforced            32m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     32m                node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  PossibleMemoryBackedVolumesOnDisk  31m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           31m                kubelet          Starting kubelet.
  Warning  CgroupV1                           31m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            31m (x8 over 31m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              31m (x8 over 31m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               31m (x7 over 31m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            31m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     30m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[May 9 08:36] PCI: Fatal: No config space access function found
[  +0.020308] PCI: System does not support PCI
[  +0.155383] kvm: already loaded the other module
[  +0.915248] FS-Cache: Duplicate cookie detected
[  +0.000575] FS-Cache: O-cookie c=00000006 [p=00000002 fl=222 nc=0 na=1]
[  +0.000683] FS-Cache: O-cookie d=00000000e44a8c62{9P.session} n=00000000bdfa619b
[  +0.000947] FS-Cache: O-key=[10] '34323934393337343038'
[  +0.000433] FS-Cache: N-cookie c=00000007 [p=00000002 fl=2 nc=0 na=1]
[  +0.000780] FS-Cache: N-cookie d=00000000e44a8c62{9P.session} n=000000009b222b0e
[  +0.000814] FS-Cache: N-key=[10] '34323934393337343038'
[  +0.474422] WSL (1 - init(docker-desktop)) ERROR: ConfigApplyWindowsLibPath:2542: open /etc/ld.so.conf.d/ld.wsl.conf failed 2
[  +0.007603] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.240552] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.009125] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000566] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000459] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000990] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +2.394400] netlink: 'init': attribute type 4 has an invalid length.
[  +1.784862] WSL (181) ERROR: CheckConnection: getaddrinfo() failed: -5
[May 9 08:59] tmpfs: Unknown parameter 'noswap'
[May 9 09:39] tmpfs: Unknown parameter 'noswap'
[May 9 09:40] tmpfs: Unknown parameter 'noswap'
[May 9 09:41] tmpfs: Unknown parameter 'noswap'
[May 9 09:50] tmpfs: Unknown parameter 'noswap'
[  +6.026067] tmpfs: Unknown parameter 'noswap'
[May 9 09:52] tmpfs: Unknown parameter 'noswap'


==> etcd [06b0fe78c8a2] <==
{"level":"info","ts":"2025-05-09T09:52:07.302456Z","caller":"etcdserver/raft.go:540","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":491}
{"level":"info","ts":"2025-05-09T09:52:07.302725Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-05-09T09:52:07.302794Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 2"}
{"level":"info","ts":"2025-05-09T09:52:07.302821Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 2, commit: 491, applied: 0, lastindex: 491, lastterm: 2]"}
{"level":"warn","ts":"2025-05-09T09:52:07.308495Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-05-09T09:52:07.389977Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":474}
{"level":"info","ts":"2025-05-09T09:52:07.395863Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-05-09T09:52:07.399847Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2025-05-09T09:52:07.400363Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-05-09T09:52:07.400455Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-05-09T09:52:07.401194Z","caller":"etcdserver/server.go:773","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2025-05-09T09:52:07.401378Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-09T09:52:07.401898Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-09T09:52:07.402250Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-09T09:52:07.402280Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-09T09:52:07.402269Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-05-09T09:52:07.402519Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-05-09T09:52:07.403036Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-09T09:52:07.403100Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-09T09:52:07.405940Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-05-09T09:52:07.406142Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-09T09:52:07.406197Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-09T09:52:07.406916Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-05-09T09:52:07.406807Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-05-09T09:52:09.003677Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 2"}
{"level":"info","ts":"2025-05-09T09:52:09.003729Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 2"}
{"level":"info","ts":"2025-05-09T09:52:09.003772Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-05-09T09:52:09.003784Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 3"}
{"level":"info","ts":"2025-05-09T09:52:09.003796Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-05-09T09:52:09.003807Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 3"}
{"level":"info","ts":"2025-05-09T09:52:09.003812Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 3"}
{"level":"info","ts":"2025-05-09T09:52:09.012704Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-05-09T09:52:09.012779Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-05-09T09:52:09.012874Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-05-09T09:52:09.012986Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-05-09T09:52:09.013035Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-05-09T09:52:09.013516Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-09T09:52:09.013582Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-09T09:52:09.014097Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-05-09T09:52:09.014119Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-05-09T09:52:15.789005Z","caller":"traceutil/trace.go:171","msg":"trace[417457240] transaction","detail":"{read_only:false; response_revision:550; number_of_response:1; }","duration":"183.128741ms","start":"2025-05-09T09:52:15.605839Z","end":"2025-05-09T09:52:15.788967Z","steps":["trace[417457240] 'process raft request'  (duration: 182.827677ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-09T09:52:15.792387Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.239909ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/kube-system/replicaset-controller\" limit:1 ","response":"range_response_count:1 size:207"}
{"level":"info","ts":"2025-05-09T09:52:15.792487Z","caller":"traceutil/trace.go:171","msg":"trace[792128722] range","detail":"{range_begin:/registry/serviceaccounts/kube-system/replicaset-controller; range_end:; response_count:1; response_revision:551; }","duration":"100.378582ms","start":"2025-05-09T09:52:15.692094Z","end":"2025-05-09T09:52:15.792472Z","steps":["trace[792128722] 'agreement among raft nodes before linearized reading'  (duration: 100.211056ms)"],"step_count":1}
{"level":"warn","ts":"2025-05-09T09:53:00.967595Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"161.970196ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-05-09T09:53:00.967667Z","caller":"traceutil/trace.go:171","msg":"trace[1438870502] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:651; }","duration":"162.088748ms","start":"2025-05-09T09:53:00.805566Z","end":"2025-05-09T09:53:00.967655Z","steps":["trace[1438870502] 'range keys from in-memory index tree'  (duration: 161.957767ms)"],"step_count":1}
{"level":"info","ts":"2025-05-09T10:02:08.992443Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":913}
{"level":"info","ts":"2025-05-09T10:02:09.000652Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":913,"took":"7.988683ms","hash":83166068,"current-db-size-bytes":3145728,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":3145728,"current-db-size-in-use":"3.1 MB"}
{"level":"info","ts":"2025-05-09T10:02:09.000729Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":83166068,"revision":913,"compact-revision":-1}
{"level":"info","ts":"2025-05-09T10:07:08.982397Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1163}
{"level":"info","ts":"2025-05-09T10:07:08.985063Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1163,"took":"2.41916ms","hash":2556989402,"current-db-size-bytes":3145728,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1753088,"current-db-size-in-use":"1.8 MB"}
{"level":"info","ts":"2025-05-09T10:07:08.985162Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":2556989402,"revision":1163,"compact-revision":913}
{"level":"info","ts":"2025-05-09T10:12:08.970401Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1409}
{"level":"info","ts":"2025-05-09T10:12:08.972729Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1409,"took":"2.185825ms","hash":3531993091,"current-db-size-bytes":3145728,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1728512,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-05-09T10:12:08.972769Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3531993091,"revision":1409,"compact-revision":1163}
{"level":"info","ts":"2025-05-09T10:17:08.960748Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1652}
{"level":"info","ts":"2025-05-09T10:17:08.967949Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1652,"took":"6.50046ms","hash":1529789719,"current-db-size-bytes":3145728,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1740800,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-05-09T10:17:08.968002Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1529789719,"revision":1652,"compact-revision":1409}
{"level":"info","ts":"2025-05-09T10:22:08.941325Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1895}
{"level":"info","ts":"2025-05-09T10:22:08.944521Z","caller":"mvcc/kvstore_compaction.go:72","msg":"finished scheduled compaction","compact-revision":1895,"took":"2.976227ms","hash":3583825559,"current-db-size-bytes":3145728,"current-db-size":"3.1 MB","current-db-size-in-use-bytes":1728512,"current-db-size-in-use":"1.7 MB"}
{"level":"info","ts":"2025-05-09T10:22:08.944587Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3583825559,"revision":1895,"compact-revision":1652}


==> etcd [7101afa9e577] <==
{"level":"info","ts":"2025-05-09T09:50:46.918912Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"warn","ts":"2025-05-09T09:50:46.919063Z","caller":"embed/config.go:689","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2025-05-09T09:50:46.919121Z","caller":"embed/etcd.go:128","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-05-09T09:50:46.919180Z","caller":"embed/etcd.go:497","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-05-09T09:50:46.920209Z","caller":"embed/etcd.go:136","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2025-05-09T09:50:46.920633Z","caller":"embed/etcd.go:311","msg":"starting an etcd server","etcd-version":"3.5.16","git-sha":"f20bbad","go-version":"go1.22.7","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":false,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"minikube=https://192.168.49.2:2380","initial-cluster-state":"new","initial-cluster-token":"etcd-cluster","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2025-05-09T09:50:46.927060Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"4.648022ms"}
{"level":"info","ts":"2025-05-09T09:50:46.932576Z","caller":"etcdserver/raft.go:505","msg":"starting local member","local-member-id":"aec36adc501070cc","cluster-id":"fa54960ea34d58be"}
{"level":"info","ts":"2025-05-09T09:50:46.932686Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2025-05-09T09:50:46.932712Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 0"}
{"level":"info","ts":"2025-05-09T09:50:46.932722Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]"}
{"level":"info","ts":"2025-05-09T09:50:46.932729Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 1"}
{"level":"info","ts":"2025-05-09T09:50:46.932760Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"warn","ts":"2025-05-09T09:50:47.000352Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2025-05-09T09:50:47.003967Z","caller":"mvcc/kvstore.go:423","msg":"kvstore restored","current-rev":1}
{"level":"info","ts":"2025-05-09T09:50:47.008367Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2025-05-09T09:50:47.011310Z","caller":"etcdserver/server.go:873","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.16","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-05-09T09:50:47.012383Z","caller":"etcdserver/server.go:757","msg":"started as single-node; fast-forwarding election ticks","local-member-id":"aec36adc501070cc","forward-ticks":9,"forward-duration":"900ms","election-ticks":10,"election-timeout":"1s"}
{"level":"info","ts":"2025-05-09T09:50:47.012421Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-09T09:50:47.012530Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-09T09:50:47.012591Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-09T09:50:47.012604Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-05-09T09:50:47.014940Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2025-05-09T09:50:47.017705Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2025-05-09T09:50:47.018973Z","caller":"embed/etcd.go:729","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-05-09T09:50:47.019180Z","caller":"embed/etcd.go:600","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-09T09:50:47.019226Z","caller":"embed/etcd.go:572","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-09T09:50:47.019902Z","caller":"embed/etcd.go:871","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-05-09T09:50:47.019784Z","caller":"embed/etcd.go:280","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-05-09T09:50:47.533673Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 1"}
{"level":"info","ts":"2025-05-09T09:50:47.533769Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 1"}
{"level":"info","ts":"2025-05-09T09:50:47.533846Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 1"}
{"level":"info","ts":"2025-05-09T09:50:47.533870Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 2"}
{"level":"info","ts":"2025-05-09T09:50:47.533879Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-05-09T09:50:47.533892Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 2"}
{"level":"info","ts":"2025-05-09T09:50:47.533901Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 2"}
{"level":"info","ts":"2025-05-09T09:50:47.546710Z","caller":"etcdserver/server.go:2651","msg":"setting up initial cluster version using v2 API","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-09T09:50:47.547900Z","caller":"etcdserver/server.go:2140","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2025-05-09T09:50:47.547966Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-05-09T09:50:47.548088Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-05-09T09:50:47.548442Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-05-09T09:50:47.548514Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-05-09T09:50:47.548676Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-09T09:50:47.548783Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-09T09:50:47.548676Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-05-09T09:50:47.550210Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-09T09:50:47.550292Z","caller":"etcdserver/server.go:2675","msg":"cluster version is updated","cluster-version":"3.5"}
{"level":"info","ts":"2025-05-09T09:50:47.594834Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2025-05-09T09:50:47.594845Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2025-05-09T09:50:53.111355Z","caller":"traceutil/trace.go:171","msg":"trace[2132633150] transaction","detail":"{read_only:false; number_of_response:0; response_revision:267; }","duration":"100.357209ms","start":"2025-05-09T09:50:53.010970Z","end":"2025-05-09T09:50:53.111327Z","steps":["trace[2132633150] 'process raft request'  (duration: 83.658126ms)"],"step_count":1}
{"level":"info","ts":"2025-05-09T09:51:25.592628Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-05-09T09:51:25.592709Z","caller":"embed/etcd.go:378","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2025-05-09T09:51:25.592840Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-09T09:51:25.592947Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-09T09:51:25.692773Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-05-09T09:51:25.692829Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-05-09T09:51:25.697066Z","caller":"etcdserver/server.go:1543","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2025-05-09T09:51:25.704333Z","caller":"embed/etcd.go:582","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-09T09:51:25.706042Z","caller":"embed/etcd.go:587","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2025-05-09T09:51:25.706100Z","caller":"embed/etcd.go:380","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 10:23:14 up  1:46,  0 users,  load average: 0.13, 0.31, 0.41
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [84c7380bc1de] <==
I0509 09:51:25.602298       1 controller.go:176] quota evaluator worker shutdown
I0509 09:51:25.602310       1 controller.go:176] quota evaluator worker shutdown
I0509 09:51:25.602317       1 controller.go:176] quota evaluator worker shutdown
I0509 09:51:25.602321       1 controller.go:176] quota evaluator worker shutdown
I0509 09:51:25.602324       1 controller.go:176] quota evaluator worker shutdown
W0509 09:51:25.603651       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.596766       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.596973       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.597065       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.597199       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.597259       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.597302       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.597262       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.597273       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.597398       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.597377       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.597387       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.597458       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.597497       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.597507       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.597508       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.598735       1 logging.go:55] [core] [Channel #17 SubChannel #18]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.598853       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.598874       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.598898       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.598911       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.598908       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.599060       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.599069       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.599089       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.599103       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.599103       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.599118       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.599139       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.599169       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.599303       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.599361       1 logging.go:55] [core] [Channel #5 SubChannel #6]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.599354       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.599462       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602187       1 logging.go:55] [core] [Channel #127 SubChannel #128]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602187       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602198       1 logging.go:55] [core] [Channel #151 SubChannel #152]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602327       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602187       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602215       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602363       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602390       1 logging.go:55] [core] [Channel #49 SubChannel #50]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602393       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602400       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602403       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602408       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602422       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602446       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602454       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602479       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602789       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602893       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.602940       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.603234       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0509 09:51:26.606948       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [ebad1b6703cc] <==
E0509 09:52:10.591013       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
I0509 09:52:10.591839       1 controller.go:615] quota admission added evaluator for: serviceaccounts
I0509 09:52:11.111872       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0509 09:52:12.501721       1 controller.go:615] quota admission added evaluator for: endpoints
I0509 09:52:15.802938       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0509 09:52:15.807421       1 controller.go:615] quota admission added evaluator for: replicasets.apps
I0509 09:52:15.896255       1 controller.go:615] quota admission added evaluator for: deployments.apps
I0509 09:52:19.001954       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
W0509 09:52:19.006768       1 handler_proxy.go:99] no RequestInfo found in the context
E0509 09:52:19.006837       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0509 09:52:19.009130       1 handler_proxy.go:143] error resolving kube-system/metrics-server: service "metrics-server" not found
I0509 09:52:19.095335       1 controller.go:615] quota admission added evaluator for: rolebindings.rbac.authorization.k8s.io
I0509 09:52:19.290500       1 alloc.go:330] "allocated clusterIPs" service="kube-system/metrics-server" clusterIPs={"IPv4":"10.98.44.199"}
W0509 09:52:19.313202       1 handler_proxy.go:99] no RequestInfo found in the context
E0509 09:52:19.313318       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0509 09:52:19.317661       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: Operation cannot be fulfilled on apiservices.apiregistration.k8s.io \"v1beta1.metrics.k8s.io\": the object has been modified; please apply your changes to the latest version and try again" logger="UnhandledError"
W0509 09:52:19.327544       1 handler_proxy.go:99] no RequestInfo found in the context
E0509 09:52:19.327614       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0509 09:52:20.002275       1 handler_proxy.go:99] no RequestInfo found in the context
W0509 09:52:20.002294       1 handler_proxy.go:99] no RequestInfo found in the context
E0509 09:52:20.002398       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
E0509 09:52:20.002452       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0509 09:52:20.003687       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0509 09:52:20.003700       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0509 09:52:58.020854       1 status.go:71] "Unhandled Error" err="apiserver received an error that is not an metav1.Status: &errors.errorString{s:\"context canceled\"}: context canceled" logger="UnhandledError"
E0509 09:53:02.258170       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.98.44.199:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.98.44.199:443/apis/metrics.k8s.io/v1beta1\": dial tcp 10.98.44.199:443: connect: connection refused" logger="UnhandledError"
W0509 09:53:02.258364       1 handler_proxy.go:99] no RequestInfo found in the context
E0509 09:53:02.258505       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
W0509 09:53:03.260432       1 handler_proxy.go:99] no RequestInfo found in the context
W0509 09:53:03.260432       1 handler_proxy.go:99] no RequestInfo found in the context
E0509 09:53:03.260576       1 controller.go:102] "Unhandled Error" err=<
	loading OpenAPI spec for "v1beta1.metrics.k8s.io" failed with: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
E0509 09:53:03.260616       1 controller.go:113] "Unhandled Error" err="loading OpenAPI spec for \"v1beta1.metrics.k8s.io\" failed with: Error, could not get list of group versions for APIService" logger="UnhandledError"
I0509 09:53:03.261775       1 controller.go:126] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
I0509 09:53:03.261815       1 controller.go:109] OpenAPI AggregationController: action for item v1beta1.metrics.k8s.io: Rate Limited Requeue.
E0509 09:53:07.265324       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.98.44.199:443/apis/metrics.k8s.io/v1beta1: Get \"https://10.98.44.199:443/apis/metrics.k8s.io/v1beta1\": context deadline exceeded" logger="UnhandledError"
W0509 09:53:07.265372       1 handler_proxy.go:99] no RequestInfo found in the context
E0509 09:53:07.265427       1 controller.go:146] "Unhandled Error" err=<
	Error updating APIService "v1beta1.metrics.k8s.io" with err: failed to download v1beta1.metrics.k8s.io: failed to retrieve openAPI spec, http error: ResponseCode: 503, Body: service unavailable
	, Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]
 > logger="UnhandledError"
I0509 09:53:07.276403       1 handler.go:286] Adding GroupVersion metrics.k8s.io v1beta1 to ResourceManager
E0509 09:53:07.282445       1 remote_available_controller.go:448] "Unhandled Error" err="v1beta1.metrics.k8s.io failed with: Operation cannot be fulfilled on apiservices.apiregistration.k8s.io \"v1beta1.metrics.k8s.io\": the object has been modified; please apply your changes to the latest version and try again" logger="UnhandledError"


==> kube-controller-manager [836ce8384064] <==
I0509 09:50:55.857163       1 shared_informer.go:320] Caches are synced for namespace
I0509 09:50:55.861279       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0509 09:50:55.861310       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0509 09:50:55.862538       1 shared_informer.go:320] Caches are synced for HPA
I0509 09:50:55.863673       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0509 09:50:55.863759       1 shared_informer.go:320] Caches are synced for PV protection
I0509 09:50:55.863781       1 shared_informer.go:320] Caches are synced for PVC protection
I0509 09:50:55.863806       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0509 09:50:55.865017       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0509 09:50:55.865527       1 shared_informer.go:320] Caches are synced for node
I0509 09:50:55.865563       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0509 09:50:55.865628       1 shared_informer.go:320] Caches are synced for persistent volume
I0509 09:50:55.865643       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0509 09:50:55.865882       1 shared_informer.go:320] Caches are synced for endpoint
I0509 09:50:55.866069       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0509 09:50:55.866140       1 shared_informer.go:320] Caches are synced for resource quota
I0509 09:50:55.866440       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0509 09:50:55.866467       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0509 09:50:55.866474       1 shared_informer.go:320] Caches are synced for cidrallocator
I0509 09:50:55.873727       1 shared_informer.go:320] Caches are synced for garbage collector
I0509 09:50:55.873781       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0509 09:50:55.873829       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0509 09:50:55.876755       1 range_allocator.go:428] "Set node PodCIDR" logger="node-ipam-controller" node="minikube" podCIDRs=["10.244.0.0/24"]
I0509 09:50:55.876814       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 09:50:55.876877       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 09:50:55.879244       1 shared_informer.go:320] Caches are synced for garbage collector
I0509 09:50:55.880656       1 shared_informer.go:320] Caches are synced for daemon sets
I0509 09:50:56.572715       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 09:50:56.987342       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="367.804551ms"
I0509 09:50:56.997688       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="10.219448ms"
I0509 09:50:56.997842       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="58.484µs"
I0509 09:50:57.012413       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="109.111µs"
I0509 09:50:59.251976       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="312.064µs"
I0509 09:51:02.493714       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 09:51:21.291496       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="11.839542ms"
I0509 09:51:21.291855       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="62.4µs"
I0509 09:51:21.583669       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="21.238152ms"
I0509 09:51:21.606505       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="21.828232ms"
I0509 09:51:21.606668       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="54.012µs"
I0509 09:51:23.029940       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 09:51:24.415692       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="17.606249ms"
E0509 09:51:24.415789       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I0509 09:51:24.427419       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="10.265742ms"
E0509 09:51:24.427463       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I0509 09:51:24.434076       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="23.125272ms"
E0509 09:51:24.434190       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-7779f9b69b\" failed with pods \"kubernetes-dashboard-7779f9b69b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I0509 09:51:24.435474       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="6.750084ms"
E0509 09:51:24.435574       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I0509 09:51:24.504255       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="68.884789ms"
E0509 09:51:24.504291       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/kubernetes-dashboard-7779f9b69b\" failed with pods \"kubernetes-dashboard-7779f9b69b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I0509 09:51:24.504255       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="65.485008ms"
E0509 09:51:24.504338       1 replica_set.go:560] "Unhandled Error" err="sync \"kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b\" failed with pods \"dashboard-metrics-scraper-5d59dccf9b-\" is forbidden: error looking up service account kubernetes-dashboard/kubernetes-dashboard: serviceaccount \"kubernetes-dashboard\" not found" logger="UnhandledError"
I0509 09:51:24.525574       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="19.726785ms"
I0509 09:51:24.529981       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="4.349501ms"
I0509 09:51:24.530121       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="68.849µs"
I0509 09:51:24.599087       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="142.066µs"
I0509 09:51:24.617055       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="70.982762ms"
I0509 09:51:24.627121       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="9.948605ms"
I0509 09:51:24.627214       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="45.579µs"
I0509 09:51:24.631388       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="50.918µs"


==> kube-controller-manager [b1a7ee98ce24] <==
I0509 09:52:15.689621       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="33.235µs"
I0509 09:52:15.689647       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="31.718µs"
I0509 09:52:15.689743       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="77.102µs"
I0509 09:52:15.689997       1 shared_informer.go:320] Caches are synced for ephemeral
I0509 09:52:15.690220       1 shared_informer.go:320] Caches are synced for resource quota
I0509 09:52:15.692534       1 shared_informer.go:320] Caches are synced for persistent volume
I0509 09:52:15.694724       1 shared_informer.go:320] Caches are synced for garbage collector
I0509 09:52:15.694796       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0509 09:52:15.694816       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0509 09:52:15.697989       1 shared_informer.go:320] Caches are synced for ReplicationController
I0509 09:52:15.698037       1 shared_informer.go:320] Caches are synced for attach detach
I0509 09:52:15.698279       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0509 09:52:15.701937       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0509 09:52:15.702401       1 shared_informer.go:320] Caches are synced for GC
I0509 09:52:15.788860       1 shared_informer.go:320] Caches are synced for garbage collector
I0509 09:52:15.815198       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="125.595945ms"
I0509 09:52:15.815330       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="41.198µs"
I0509 09:52:19.089821       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="60.171334ms"
I0509 09:52:19.097808       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="7.910549ms"
I0509 09:52:19.098078       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="76.123µs"
I0509 09:52:19.107219       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="64.299µs"
I0509 09:52:26.443771       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="8.003581ms"
I0509 09:52:26.443975       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/kubernetes-dashboard-7779f9b69b" duration="118.627µs"
I0509 09:52:39.225392       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="11.902668ms"
I0509 09:52:39.225522       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-668d6bf9bc" duration="61.718µs"
I0509 09:52:41.558997       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
E0509 09:52:45.696824       1 resource_quota_controller.go:446] "Unhandled Error" err="unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1: stale GroupVersion discovery: metrics.k8s.io/v1beta1" logger="UnhandledError"
I0509 09:52:45.803372       1 garbagecollector.go:787] "failed to discover some groups" logger="garbage-collector-controller" groups="map[\"metrics.k8s.io/v1beta1\":\"stale GroupVersion discovery: metrics.k8s.io/v1beta1\"]"
I0509 09:52:49.020500       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="44.577µs"
I0509 09:52:55.088090       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="7.667769ms"
I0509 09:52:55.088188       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kubernetes-dashboard/dashboard-metrics-scraper-5d59dccf9b" duration="48.797µs"
I0509 09:53:01.235309       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="91.449µs"
I0509 09:53:02.251302       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="7.023321ms"
I0509 09:53:02.251520       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/metrics-server-7fbb699795" duration="121.618µs"
I0509 09:53:04.268996       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="75.488µs"
I0509 09:53:05.294484       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="40.681µs"
I0509 09:53:12.661678       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 09:53:22.470713       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="52.017µs"
I0509 09:53:37.300540       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="102.25µs"
I0509 09:53:53.707104       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="37.467µs"
I0509 09:54:09.302468       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="57.331µs"
I0509 09:54:41.055337       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="31.695µs"
I0509 09:54:55.298153       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="57.773µs"
I0509 09:56:13.795921       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="130.193µs"
I0509 09:56:24.296257       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="123.857µs"
I0509 09:59:04.144921       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="91.873µs"
I0509 09:59:19.283152       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="47.004µs"
I0509 10:00:21.174898       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 10:04:19.389806       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="90.923µs"
I0509 10:04:34.262649       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="85.403µs"
I0509 10:05:26.936651       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 10:09:32.575564       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="40.869µs"
I0509 10:09:44.235610       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="58.247µs"
I0509 10:10:33.116150       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 10:14:37.785821       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="179.757µs"
I0509 10:14:53.220508       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="190.735µs"
I0509 10:15:39.011039       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"
I0509 10:19:44.359898       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="102.087µs"
I0509 10:19:55.210036       1 replica_set.go:679] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/kmyclassjar-deployment-5c86db7f6b" duration="63.06µs"
I0509 10:20:46.696341       1 range_allocator.go:247] "Successfully synced" logger="node-ipam-controller" key="minikube"


==> kube-proxy [d4febcd81c67] <==
I0509 09:50:57.931157       1 server_linux.go:66] "Using iptables proxy"
I0509 09:50:58.278084       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0509 09:50:58.278202       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0509 09:50:58.312522       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0509 09:50:58.312588       1 server_linux.go:170] "Using iptables Proxier"
I0509 09:50:58.315501       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0509 09:50:58.328508       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0509 09:50:58.345856       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0509 09:50:58.346088       1 server.go:497] "Version info" version="v1.32.0"
I0509 09:50:58.346174       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0509 09:50:58.361594       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0509 09:50:58.374279       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0509 09:50:58.375918       1 config.go:329] "Starting node config controller"
I0509 09:50:58.376032       1 shared_informer.go:313] Waiting for caches to sync for node config
I0509 09:50:58.375893       1 config.go:105] "Starting endpoint slice config controller"
I0509 09:50:58.376149       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0509 09:50:58.375924       1 config.go:199] "Starting service config controller"
I0509 09:50:58.376175       1 shared_informer.go:313] Waiting for caches to sync for service config
I0509 09:50:58.476674       1 shared_informer.go:320] Caches are synced for service config
I0509 09:50:58.476701       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0509 09:50:58.476764       1 shared_informer.go:320] Caches are synced for node config


==> kube-proxy [fad65ca5223e] <==
I0509 09:52:13.897158       1 server_linux.go:66] "Using iptables proxy"
I0509 09:52:14.306909       1 server.go:698] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
E0509 09:52:14.307043       1 server.go:234] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0509 09:52:14.501346       1 server.go:243] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0509 09:52:14.501428       1 server_linux.go:170] "Using iptables Proxier"
I0509 09:52:14.507593       1 proxier.go:255] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0509 09:52:14.517552       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0509 09:52:14.606026       1 proxier.go:283] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0509 09:52:14.606221       1 server.go:497] "Version info" version="v1.32.0"
I0509 09:52:14.606246       1 server.go:499] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0509 09:52:14.623464       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0509 09:52:14.637871       1 metrics.go:340] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0509 09:52:14.639265       1 config.go:199] "Starting service config controller"
I0509 09:52:14.639314       1 shared_informer.go:313] Waiting for caches to sync for service config
I0509 09:52:14.639335       1 config.go:105] "Starting endpoint slice config controller"
I0509 09:52:14.639338       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0509 09:52:14.640004       1 config.go:329] "Starting node config controller"
I0509 09:52:14.640011       1 shared_informer.go:313] Waiting for caches to sync for node config
I0509 09:52:14.788090       1 shared_informer.go:320] Caches are synced for node config
I0509 09:52:14.788098       1 shared_informer.go:320] Caches are synced for service config
I0509 09:52:14.788144       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-scheduler [3d1be100257b] <==
I0509 09:52:08.030163       1 serving.go:386] Generated self-signed cert in-memory
W0509 09:52:10.100972       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0509 09:52:10.101033       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0509 09:52:10.101047       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0509 09:52:10.101056       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0509 09:52:10.401916       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0509 09:52:10.401960       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0509 09:52:10.490481       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0509 09:52:10.490595       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0509 09:52:10.490743       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0509 09:52:10.490794       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0509 09:52:10.593577       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file


==> kube-scheduler [5a6d22d5e907] <==
I0509 09:50:47.994915       1 serving.go:386] Generated self-signed cert in-memory
W0509 09:50:49.297054       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0509 09:50:49.297090       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0509 09:50:49.297103       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0509 09:50:49.297111       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0509 09:50:49.329269       1 server.go:166] "Starting Kubernetes Scheduler" version="v1.32.0"
I0509 09:50:49.329363       1 server.go:168] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0509 09:50:49.331138       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0509 09:50:49.331215       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0509 09:50:49.331230       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0509 09:50:49.331240       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
W0509 09:50:49.397189       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0509 09:50:49.397270       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
W0509 09:50:49.398187       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0509 09:50:49.398293       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0509 09:50:49.398364       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0509 09:50:49.398364       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csistoragecapacities\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0509 09:50:49.398413       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0509 09:50:49.399450       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0509 09:50:49.398246       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0509 09:50:49.399489       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0509 09:50:49.398413       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0509 09:50:49.399493       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0509 09:50:49.399514       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"statefulsets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0509 09:50:49.398457       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0509 09:50:49.399533       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User \"system:kube-scheduler\" cannot list resource \"namespaces\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0509 09:50:49.399544       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"storageclasses\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0509 09:50:49.398469       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0509 09:50:49.399558       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User \"system:kube-scheduler\" cannot list resource \"nodes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 09:50:49.398528       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csidrivers" in API group "storage.k8s.io" at the cluster scope
E0509 09:50:49.399571       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: csidrivers.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csidrivers\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0509 09:50:49.398585       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0509 09:50:49.399583       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 09:50:49.398580       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E0509 09:50:49.399595       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Pod: failed to list *v1.Pod: pods is forbidden: User \"system:kube-scheduler\" cannot list resource \"pods\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 09:50:49.398582       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0509 09:50:49.399629       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0509 09:50:49.399137       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E0509 09:50:49.399680       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumes\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 09:50:49.400962       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W0509 09:50:49.401039       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E0509 09:50:49.401086       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicationcontrollers\" in API group \"\" at the cluster scope" logger="UnhandledError"
E0509 09:50:49.401068       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"csinodes\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0509 09:50:50.211722       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E0509 09:50:50.211780       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User \"system:kube-scheduler\" cannot list resource \"poddisruptionbudgets\" in API group \"policy\" at the cluster scope" logger="UnhandledError"
W0509 09:50:50.374302       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0509 09:50:50.374365       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User \"system:kube-scheduler\" cannot list resource \"services\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 09:50:50.388816       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0509 09:50:50.388856       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User \"system:kube-scheduler\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope" logger="UnhandledError"
W0509 09:50:50.476425       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0509 09:50:50.476479       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User \"system:kube-scheduler\" cannot list resource \"persistentvolumeclaims\" in API group \"\" at the cluster scope" logger="UnhandledError"
W0509 09:50:50.534906       1 reflector.go:569] k8s.io/client-go/informers/factory.go:160: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "volumeattachments" in API group "storage.k8s.io" at the cluster scope
E0509 09:50:50.535064       1 reflector.go:166] "Unhandled Error" err="k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.VolumeAttachment: failed to list *v1.VolumeAttachment: volumeattachments.storage.k8s.io is forbidden: User \"system:kube-scheduler\" cannot list resource \"volumeattachments\" in API group \"storage.k8s.io\" at the cluster scope" logger="UnhandledError"
W0509 09:50:50.594772       1 reflector.go:569] runtime/asm_amd64.s:1700: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0509 09:50:50.594811       1 reflector.go:166] "Unhandled Error" err="runtime/asm_amd64.s:1700: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps \"extension-apiserver-authentication\" is forbidden: User \"system:kube-scheduler\" cannot list resource \"configmaps\" in API group \"\" in the namespace \"kube-system\"" logger="UnhandledError"
I0509 09:50:53.431424       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
E0509 09:51:25.593559       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
May 09 10:17:05 minikube kubelet[1627]: I0509 10:17:05.197200    1627 scope.go:117] "RemoveContainer" containerID="da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c"
May 09 10:17:05 minikube kubelet[1627]: E0509 10:17:05.197410    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:17:16 minikube kubelet[1627]: I0509 10:17:16.197751    1627 scope.go:117] "RemoveContainer" containerID="da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c"
May 09 10:17:16 minikube kubelet[1627]: E0509 10:17:16.197959    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:17:29 minikube kubelet[1627]: I0509 10:17:29.194876    1627 scope.go:117] "RemoveContainer" containerID="da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c"
May 09 10:17:29 minikube kubelet[1627]: E0509 10:17:29.195148    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:17:43 minikube kubelet[1627]: I0509 10:17:43.194838    1627 scope.go:117] "RemoveContainer" containerID="da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c"
May 09 10:17:43 minikube kubelet[1627]: E0509 10:17:43.195064    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:17:55 minikube kubelet[1627]: I0509 10:17:55.209594    1627 scope.go:117] "RemoveContainer" containerID="da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c"
May 09 10:17:55 minikube kubelet[1627]: E0509 10:17:55.209739    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:18:06 minikube kubelet[1627]: I0509 10:18:06.193241    1627 scope.go:117] "RemoveContainer" containerID="da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c"
May 09 10:18:06 minikube kubelet[1627]: E0509 10:18:06.193423    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:18:19 minikube kubelet[1627]: I0509 10:18:19.194073    1627 scope.go:117] "RemoveContainer" containerID="da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c"
May 09 10:18:19 minikube kubelet[1627]: E0509 10:18:19.194443    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:18:30 minikube kubelet[1627]: I0509 10:18:30.190913    1627 scope.go:117] "RemoveContainer" containerID="da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c"
May 09 10:18:30 minikube kubelet[1627]: E0509 10:18:30.191229    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:18:42 minikube kubelet[1627]: I0509 10:18:42.190220    1627 scope.go:117] "RemoveContainer" containerID="da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c"
May 09 10:18:42 minikube kubelet[1627]: E0509 10:18:42.190421    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:18:54 minikube kubelet[1627]: I0509 10:18:54.191984    1627 scope.go:117] "RemoveContainer" containerID="da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c"
May 09 10:18:54 minikube kubelet[1627]: E0509 10:18:54.192149    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:19:05 minikube kubelet[1627]: I0509 10:19:05.186626    1627 scope.go:117] "RemoveContainer" containerID="da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c"
May 09 10:19:05 minikube kubelet[1627]: E0509 10:19:05.186824    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:19:18 minikube kubelet[1627]: I0509 10:19:18.186747    1627 scope.go:117] "RemoveContainer" containerID="da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c"
May 09 10:19:18 minikube kubelet[1627]: E0509 10:19:18.186912    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:19:30 minikube kubelet[1627]: I0509 10:19:30.184244    1627 scope.go:117] "RemoveContainer" containerID="da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c"
May 09 10:19:30 minikube kubelet[1627]: E0509 10:19:30.184402    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:19:41 minikube kubelet[1627]: I0509 10:19:41.183988    1627 scope.go:117] "RemoveContainer" containerID="da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c"
May 09 10:19:44 minikube kubelet[1627]: I0509 10:19:44.349924    1627 scope.go:117] "RemoveContainer" containerID="da265c41b845ccd79ac733aadabd6adc9b3add45e017376ed77cb1a00847182c"
May 09 10:19:44 minikube kubelet[1627]: I0509 10:19:44.350271    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:19:44 minikube kubelet[1627]: E0509 10:19:44.350386    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:19:55 minikube kubelet[1627]: I0509 10:19:55.184854    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:19:55 minikube kubelet[1627]: E0509 10:19:55.185408    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:20:07 minikube kubelet[1627]: I0509 10:20:07.180649    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:20:07 minikube kubelet[1627]: E0509 10:20:07.180841    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:20:22 minikube kubelet[1627]: I0509 10:20:22.180335    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:20:22 minikube kubelet[1627]: E0509 10:20:22.180523    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:20:36 minikube kubelet[1627]: I0509 10:20:36.176700    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:20:36 minikube kubelet[1627]: E0509 10:20:36.177342    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:20:50 minikube kubelet[1627]: I0509 10:20:50.176886    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:20:50 minikube kubelet[1627]: E0509 10:20:50.177249    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:21:05 minikube kubelet[1627]: I0509 10:21:05.173842    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:21:05 minikube kubelet[1627]: E0509 10:21:05.173976    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:21:20 minikube kubelet[1627]: I0509 10:21:20.173949    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:21:20 minikube kubelet[1627]: E0509 10:21:20.174108    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:21:34 minikube kubelet[1627]: I0509 10:21:34.169763    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:21:34 minikube kubelet[1627]: E0509 10:21:34.169951    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:21:47 minikube kubelet[1627]: I0509 10:21:47.169906    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:21:47 minikube kubelet[1627]: E0509 10:21:47.170120    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:22:01 minikube kubelet[1627]: I0509 10:22:01.166993    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:22:01 minikube kubelet[1627]: E0509 10:22:01.167137    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:22:12 minikube kubelet[1627]: I0509 10:22:12.167407    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:22:12 minikube kubelet[1627]: E0509 10:22:12.167588    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:22:24 minikube kubelet[1627]: I0509 10:22:24.167309    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:22:24 minikube kubelet[1627]: E0509 10:22:24.167522    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:22:38 minikube kubelet[1627]: I0509 10:22:38.163921    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:22:38 minikube kubelet[1627]: E0509 10:22:38.164202    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:22:52 minikube kubelet[1627]: I0509 10:22:52.163009    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:22:52 minikube kubelet[1627]: E0509 10:22:52.163177    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"
May 09 10:23:07 minikube kubelet[1627]: I0509 10:23:07.161339    1627 scope.go:117] "RemoveContainer" containerID="d7af487ae425fbf06cbb4cca9c381552968b8a18b55aba456cee6a7c1e7dab1d"
May 09 10:23:07 minikube kubelet[1627]: E0509 10:23:07.161474    1627 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"kmyclassjar\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=kmyclassjar pod=kmyclassjar-deployment-5c86db7f6b-8x62f_default(4751915f-1fe6-4a78-98f4-540e7f0dd17a)\"" pod="default/kmyclassjar-deployment-5c86db7f6b-8x62f" podUID="4751915f-1fe6-4a78-98f4-540e7f0dd17a"


==> kubernetes-dashboard [8aa852385f55] <==
2025/05/09 10:08:10 Getting list of all cron jobs in the cluster
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/09 10:08:10 Getting list of all deployments in the cluster
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/09 10:08:10 Getting list of all jobs in the cluster
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/09 10:08:10 Getting list of all pods in the cluster
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/09 10:08:10 received 0 resources from sidecar instead of 1
2025/05/09 10:08:10 received 0 resources from sidecar instead of 1
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Incoming HTTP/1.1 GET /api/v1/replicaset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/09 10:08:10 Getting list of all replica sets in the cluster
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Incoming HTTP/1.1 GET /api/v1/statefulset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/09 10:08:10 Getting list of all pet sets in the cluster
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Incoming HTTP/1.1 GET /api/v1/replicationcontroller/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/09 10:08:10 Getting list of all replication controllers in the cluster
2025/05/09 10:08:10 received 0 resources from sidecar instead of 1
2025/05/09 10:08:10 Getting pod metrics
2025/05/09 10:08:10 received 0 resources from sidecar instead of 1
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/09 10:08:10 received 0 resources from sidecar instead of 1
2025/05/09 10:08:10 received 0 resources from sidecar instead of 1
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/09 10:08:10 received 0 resources from sidecar instead of 1
2025/05/09 10:08:10 received 0 resources from sidecar instead of 1
2025/05/09 10:08:10 Skipping metric because of error: Metric label not set.
2025/05/09 10:08:10 Skipping metric because of error: Metric label not set.
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/09 10:08:10 [2025-05-09T10:08:10Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/09 10:08:11 [2025-05-09T10:08:11Z] Incoming HTTP/1.1 GET /api/v1/namespace request from 127.0.0.1: 
2025/05/09 10:08:11 Getting list of namespaces
2025/05/09 10:08:11 [2025-05-09T10:08:11Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/09 10:08:11 [2025-05-09T10:08:11Z] Incoming HTTP/1.1 GET /api/v1/deployment/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/09 10:08:11 Getting list of all deployments in the cluster
2025/05/09 10:08:11 [2025-05-09T10:08:11Z] Incoming HTTP/1.1 GET /api/v1/cronjob/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/09 10:08:11 Getting list of all cron jobs in the cluster
2025/05/09 10:08:11 [2025-05-09T10:08:11Z] Incoming HTTP/1.1 GET /api/v1/daemonset/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/09 10:08:11 [2025-05-09T10:08:11Z] Incoming HTTP/1.1 GET /api/v1/job/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/09 10:08:11 Getting list of all jobs in the cluster
2025/05/09 10:08:11 [2025-05-09T10:08:11Z] Incoming HTTP/1.1 GET /api/v1/pod/default?itemsPerPage=10&page=1&sortBy=d,creationTimestamp request from 127.0.0.1: 
2025/05/09 10:08:11 Getting list of all pods in the cluster
2025/05/09 10:08:11 [2025-05-09T10:08:11Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/09 10:08:11 [2025-05-09T10:08:11Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/09 10:08:11 [2025-05-09T10:08:11Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/09 10:08:11 received 0 resources from sidecar instead of 1
2025/05/09 10:08:11 received 0 resources from sidecar instead of 1
2025/05/09 10:08:11 received 0 resources from sidecar instead of 1
2025/05/09 10:08:11 [2025-05-09T10:08:11Z] Outcoming response to 127.0.0.1 with 200 status code
2025/05/09 10:08:11 received 0 resources from sidecar instead of 1
2025/05/09 10:08:11 Getting pod metrics
2025/05/09 10:08:11 received 0 resources from sidecar instead of 1
2025/05/09 10:08:11 received 0 resources from sidecar instead of 1
2025/05/09 10:08:11 Skipping metric because of error: Metric label not set.
2025/05/09 10:08:11 Skipping metric because of error: Metric label not set.
2025/05/09 10:08:11 [2025-05-09T10:08:11Z] Outcoming response to 127.0.0.1 with 200 status code


==> storage-provisioner [b2b30b78d55c] <==
I0509 09:52:13.393100       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0509 09:52:34.448766       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: connect: connection refused


==> storage-provisioner [edd4e3e79a1d] <==
I0509 09:52:46.925528       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I0509 09:52:46.993192       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I0509 09:52:46.993240       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I0509 09:53:04.388226       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I0509 09:53:04.388336       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"2e1dab28-0eba-4e5d-9d0a-1f8f5048ac91", APIVersion:"v1", ResourceVersion:"672", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_17ec9e99-5b75-4798-8a7e-67311207c2e2 became leader
I0509 09:53:04.388362       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_17ec9e99-5b75-4798-8a7e-67311207c2e2!
I0509 09:53:04.489643       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_17ec9e99-5b75-4798-8a7e-67311207c2e2!

